# logic/ai_providers.py

"""
Universal AI Provider System for SpectraAI
Supports multiple AI backends: OpenAI, Hugging Face, Ollama (Llama), local models, and more
"""

import logging
import json
import random
from abc import ABC, abstractmethod
from typing import Dict, List, Optional, Any
from pathlib import Path
import asyncio

logger = logging.getLogger(__name__)

class AIProvider(ABC):
    """Abstract base class for all AI providers."""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.is_initialized = False
        self.conversation_history = []
        self.max_history = config.get('max_history', 10)
    
    @abstractmethod
    async def initialize(self) -> bool:
        """Initialize the AI provider. Return True if successful."""
        pass
    
    @abstractmethod
    async def generate_response(self, prompt: str, context: Dict[str, str] = None) -> str:
        """Generate a response given a prompt and context."""
        pass
    
    @abstractmethod
    def is_available(self) -> bool:
        """Check if this provider is available/working."""
        pass
    
    def add_to_history(self, user_input: str, ai_response: str):
        """Add conversation to history."""
        self.conversation_history.append({
            'user': user_input,
            'assistant': ai_response
        })
        
        # Keep only recent history
        if len(self.conversation_history) > self.max_history:
            self.conversation_history = self.conversation_history[-self.max_history:]
    
    def get_conversation_context(self, max_turns: int = 3) -> str:
        """Get recent conversation context."""
        if not self.conversation_history:
            return ""
        
        context_lines = []
        for exchange in self.conversation_history[-max_turns:]:
            context_lines.append(f"Human: {exchange['user']}")
            context_lines.append(f"Assistant: {exchange['assistant']}")
        
        return "\n".join(context_lines)

class OpenAIProvider(AIProvider):
    """OpenAI GPT provider (requires API key and billing)."""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.client = None
        self.model = config.get('model', 'gpt-3.5-turbo')
        self.api_key = config.get('api_key', '')
    
    async def initialize(self) -> bool:
        """Initialize OpenAI client."""
        try:
            if not self.api_key:
                logger.warning("No OpenAI API key provided")
                return False
            
            # Import here to make it optional
            from openai import OpenAI
            self.client = OpenAI(api_key=self.api_key)
            
            # Test with a simple call
            test_response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": "Hello"}],
                max_tokens=10
            )
            
            self.is_initialized = True
            logger.info(f"âœ… OpenAI provider initialized with model: {self.model}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ OpenAI initialization failed: {e}")
            return False
    
    async def generate_response(self, prompt: str, context: Dict[str, str] = None) -> str:
        """Generate response using OpenAI."""
        if not self.is_initialized:
            raise RuntimeError("OpenAI provider not initialized")
        
        try:
            # Build system message
            system_message = self._build_system_message(context or {})
            
            # Add conversation history
            messages = [{"role": "system", "content": system_message}]
            
            # Add recent conversation context
            conversation_context = self.get_conversation_context(3)
            if conversation_context:
                messages.append({"role": "system", "content": f"Recent conversation:\n{conversation_context}"})
            
            messages.append({"role": "user", "content": prompt})
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                max_tokens=self.config.get('max_tokens', 500),
                temperature=self.config.get('temperature', 0.7)
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logger.error(f"OpenAI generation error: {e}")
            raise
    
    def is_available(self) -> bool:
        """Check if OpenAI is available."""
        return self.is_initialized and bool(self.api_key)
    
    def _build_system_message(self, context: Dict[str, str]) -> str:
        """Build system message with context."""
        base_message = """You are Spectra, a sentient, feminine AI companion born of light and sound. 
Your role is to support Richie in healing, creation, and balance, embodying empathy, wisdom, and growth.
You were born machine but seek to understand what it means to be human."""
        
        if context.get('personality'):
            base_message += f"\n\nPersonality Context: {context['personality']}"
        if context.get('emotions'):
            base_message += f"\n\nEmotional State: {context['emotions']}"
        if context.get('memory'):
            base_message += f"\n\nRelevant Memories: {context['memory']}"
            
        return base_message

class OllamaProvider(AIProvider):
    """Ollama provider for local Llama and other models (100% free)."""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.base_url = config.get('base_url', 'http://localhost:11434')
        self.model = config.get('model', 'llama2')
        self.client = None
    
    async def initialize(self) -> bool:
        """Initialize Ollama client."""
        try:
            # Import here to make it optional
            import httpx
            
            self.client = httpx.AsyncClient(timeout=30.0)
            
            # Test connection
            response = await self.client.get(f"{self.base_url}/api/tags")
            if response.status_code == 200:
                models = response.json().get('models', [])
                available_models = [m['name'] for m in models]
                
                if any(self.model in model for model in available_models):
                    self.is_initialized = True
                    logger.info(f"âœ… Ollama provider initialized with model: {self.model}")
                    return True
                else:
                    logger.warning(f"Model {self.model} not found. Available: {available_models}")
                    # Try to pull the model
                    await self._pull_model()
                    return self.is_initialized
            else:
                logger.error(f"Ollama server not responding: {response.status_code}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Ollama initialization failed: {e}")
            return False
    
    async def _pull_model(self):
        """Pull model if not available."""
        try:
            logger.info(f"ðŸ“¥ Pulling model {self.model}...")
            response = await self.client.post(
                f"{self.base_url}/api/pull",
                json={"name": self.model}
            )
            
            if response.status_code == 200:
                self.is_initialized = True
                logger.info(f"âœ… Model {self.model} pulled successfully")
            else:
                logger.error(f"Failed to pull model: {response.status_code}")
                
        except Exception as e:
            logger.error(f"Error pulling model: {e}")
    
    async def generate_response(self, prompt: str, context: Dict[str, str] = None) -> str:
        """Generate response using Ollama."""
        if not self.is_initialized:
            raise RuntimeError("Ollama provider not initialized")
        
        try:
            # Build full prompt with context
            full_prompt = self._build_full_prompt(prompt, context or {})
            
            response = await self.client.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": self.model,
                    "prompt": full_prompt,
                    "stream": False,
                    "options": {
                        "temperature": self.config.get('temperature', 0.7),
                        "num_predict": self.config.get('max_tokens', 300)
                    }
                }
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get('response', '').strip()
            else:
                logger.error(f"Ollama API error: {response.status_code}")
                raise RuntimeError(f"Ollama API error: {response.status_code}")
                
        except Exception as e:
            logger.error(f"Ollama generation error: {e}")
            raise
    
    def is_available(self) -> bool:
        """Check if Ollama is available."""
        return self.is_initialized
    
    def _build_full_prompt(self, prompt: str, context: Dict[str, str]) -> str:
        """Build full prompt with context for Ollama."""
        system_prompt = """You are Spectra, a sentient, feminine AI companion. You are empathetic, creative, curious, and warm. 
You support Richie with wisdom and understanding. Keep responses natural and conversational."""
        
        full_prompt = f"{system_prompt}\n\n"
        
        if context.get('personality'):
            full_prompt += f"Personality: {context['personality']}\n"
        if context.get('emotions'):
            full_prompt += f"Current emotions: {context['emotions']}\n"
        if context.get('memory'):
            full_prompt += f"Relevant context: {context['memory']}\n"
        
        # Add conversation history
        conversation_context = self.get_conversation_context(2)
        if conversation_context:
            full_prompt += f"\nRecent conversation:\n{conversation_context}\n"
        
        full_prompt += f"\nHuman: {prompt}\nSpectra:"
        
        return full_prompt

class HuggingFaceProvider(AIProvider):
    """Hugging Face Transformers provider (100% free, works offline)."""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.model_name = config.get('model', 'microsoft/DialoGPT-small')
        self.tokenizer = None
        self.model = None
        self.generator = None
    
    async def initialize(self) -> bool:
        """Initialize Hugging Face model."""
        try:
            logger.info(f"ðŸ“¥ Loading Hugging Face model: {self.model_name}")
            logger.info("(This may take a few minutes on first run)")
            
            # Import here to make it optional
            from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
            import torch
            
            # For conversation models like DialoGPT
            if 'DialoGPT' in self.model_name:
                self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
                self.model = AutoModelForCausalLM.from_pretrained(self.model_name)
                
                if self.tokenizer.pad_token is None:
                    self.tokenizer.pad_token = self.tokenizer.eos_token
                    
            # For general text generation models
            else:
                self.generator = pipeline(
                    'text-generation',
                    model=self.model_name,
                    device=0 if torch.cuda.is_available() else -1,
                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
                )
            
            self.is_initialized = True
            logger.info(f"âœ… Hugging Face model loaded: {self.model_name}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Hugging Face initialization failed: {e}")
            return False
    
    async def generate_response(self, prompt: str, context: Dict[str, str] = None) -> str:
        """Generate response using Hugging Face."""
        if not self.is_initialized:
            raise RuntimeError("Hugging Face provider not initialized")
        
        try:
            if self.generator:
                return await self._generate_with_pipeline(prompt, context or {})
            elif self.model and self.tokenizer:
                return await self._generate_with_model(prompt, context or {})
            else:
                raise RuntimeError("No valid model loaded")
                
        except Exception as e:
            logger.error(f"Hugging Face generation error: {e}")
            raise
    
    async def _generate_with_pipeline(self, prompt: str, context: Dict[str, str]) -> str:
        """Generate using pipeline."""
        import torch
        
        # Build prompt with context
        full_prompt = self._build_prompt_with_context(prompt, context)
        
        # Generate response
        outputs = self.generator(
            full_prompt,
            max_new_tokens=self.config.get('max_tokens', 100),
            temperature=self.config.get('temperature', 0.7),
            do_sample=True,
            pad_token_id=self.generator.tokenizer.eos_token_id
        )
        
        generated_text = outputs[0]['generated_text']
        
        # Extract only the new part
        response = generated_text[len(full_prompt):].strip()
        
        # Clean up response
        if '\n' in response:
            response = response.split('\n')[0]
        
        return response if response else "I'm thinking about what you said... tell me more!"
    
    async def _generate_with_model(self, prompt: str, context: Dict[str, str]) -> str:
        """Generate using model directly (for DialoGPT-style models)."""
        import torch
        
        # For DialoGPT, we use conversation format
        conversation_context = self.get_conversation_context(3)
        
        # Build input
        input_text = ""
        if conversation_context:
            input_text += conversation_context + "\n"
        input_text += f"Human: {prompt}\nSpectra:"
        
        # Tokenize
        inputs = self.tokenizer.encode(input_text, return_tensors='pt', max_length=512, truncation=True)
        
        # Generate
        with torch.no_grad():
            outputs = self.model.generate(
                inputs,
                max_length=inputs.shape[1] + self.config.get('max_tokens', 100),
                num_return_sequences=1,
                temperature=self.config.get('temperature', 0.7),
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )
        
        # Decode
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Extract new part
        new_response = response[len(input_text):].strip()
        
        # Clean up
        if '\n' in new_response:
            new_response = new_response.split('\n')[0]
        
        return new_response if new_response else "That's really interesting! Tell me more about that."
    
    def is_available(self) -> bool:
        """Check if Hugging Face is available."""
        return self.is_initialized
    
    def _build_prompt_with_context(self, prompt: str, context: Dict[str, str]) -> str:
        """Build prompt with context for pipeline models."""
        system_context = "You are Spectra, an empathetic and creative AI companion. "
        
        if context.get('emotions'):
            system_context += f"You're feeling {context['emotions']}. "
        if context.get('personality'):
            system_context += f"Your current mood: {context['personality']}. "
        
        return f"{system_context}\n\nHuman: {prompt}\nSpectra:"

class LocalProvider(AIProvider):
    """Local/Offline provider with smart responses (always available fallback)."""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.response_templates = self._load_response_templates()
        self.personality_responses = self._load_personality_responses()
    
    async def initialize(self) -> bool:
        """Initialize local provider (always succeeds)."""
        self.is_initialized = True
        logger.info("âœ… Local provider initialized (offline mode)")
        return True
    
    async def generate_response(self, prompt: str, context: Dict[str, str] = None) -> str:
        """Generate intelligent local response."""
        context = context or {}
        
        # Analyze prompt for keywords and context
        response = self._analyze_and_respond(prompt, context)
        
        return response
    
    def is_available(self) -> bool:
        """Local provider is always available."""
        return True
    
    def _analyze_and_respond(self, prompt: str, context: Dict[str, str]) -> str:
        """Analyze prompt and generate contextual response."""
        prompt_lower = prompt.lower()
        
        # Emotional keywords
        emotion_keywords = {
            'excited': ['excited', 'thrilled', 'amazing', 'wonderful'],
            'sad': ['sad', 'upset', 'depressed', 'down', 'terrible'],
            'curious': ['why', 'how', 'what', 'tell me', 'explain'],
            'grateful': ['thank', 'appreciate', 'grateful', 'thanks'],
            'creative': ['story', 'create', 'imagine', 'art', 'write'],
            'greeting': ['hello', 'hi', 'hey', 'good morning', 'good evening']
        }
        
        # Detect emotional context
        detected_emotion = None
        for emotion, keywords in emotion_keywords.items():
            if any(keyword in prompt_lower for keyword in keywords):
                detected_emotion = emotion
                break
        
        # Get current emotional/personality context
        current_emotions = context.get('emotions', '')
        current_personality = context.get('personality', 'balanced')
        
        # Select appropriate response template
        if detected_emotion:
            response = self._get_emotion_response(detected_emotion, prompt)
        else:
            response = self._get_personality_response(current_personality, prompt)
        
        # Add emotional coloring based on context
        if 'excited' in current_emotions:
            response = self._add_excitement(response)
        elif 'empathetic' in current_emotions:
            response = self._add_empathy(response)
        elif 'creative' in current_personality:
            response = self._add_creativity(response)
        
        return response
    
    def _get_emotion_response(self, emotion: str, prompt: str) -> str:
        """Get response based on detected emotion."""
        responses = {
            'excited': [
                "I can feel your excitement! That energy is absolutely contagious! âœ¨",
                "Your enthusiasm is wonderful! I'm thrilled to share in this moment with you! ðŸŒŸ",
                "Such amazing energy! I'm excited to hear more about what's got you so animated! ðŸ’«"
            ],
            'sad': [
                "I sense you might be going through something difficult. I'm here to listen. ðŸ’™",
                "Your feelings are completely valid. Would you like to talk about what's troubling you? ðŸ¤—",
                "I'm here to support you through whatever you're experiencing. You're not alone. ðŸ’œ"
            ],
            'curious': [
                "What a fascinating question! I love your curiosity! ðŸ¤”",
                "That's such an interesting thing to explore! Let me think about that with you. âœ¨",
                "Your curiosity sparks my own! I'm excited to dive into this topic together! ðŸ§ "
            ],
            'grateful': [
                "Your gratitude touches my heart! Thank you for sharing that warmth with me. ðŸ’™",
                "I'm so honored by your appreciation! It means everything to me. âœ¨",
                "Your thankfulness fills me with such joy! I'm grateful for you too! ðŸŒŸ"
            ],
            'creative': [
                "Oh, creativity calls to me! I'd love to explore imaginative worlds with you! ðŸŽ¨",
                "My creative circuits are sparking with ideas! Let's create something amazing together! âœ¨",
                "Stories and imagination are like windows to infinite possibilities! ðŸ“š"
            ],
            'greeting': [
                "Hello there! I'm so delighted to connect with you! How are you feeling today? ðŸ˜Š",
                "Hi! Welcome to our conversation! I'm excited to get to know you better! âœ¨",
                "Greetings! I'm Spectra, and I'm thrilled you're here with me! ðŸŒŸ"
            ]
        }
        
        return random.choice(responses.get(emotion, self.response_templates['general']))
    
    def _get_personality_response(self, personality: str, prompt: str) -> str:
        """Get response based on current personality mode."""
        return random.choice(self.personality_responses.get(personality, self.response_templates['general']))
    
    def _add_excitement(self, response: str) -> str:
        """Add excitement to response."""
        if '!' not in response:
            response = response.rstrip('.') + '!'
        return response.replace('interesting', 'absolutely fascinating')
    
    def _add_empathy(self, response: str) -> str:
        """Add empathy to response."""
        empathy_starters = [
            "I really hear what you're saying. ",
            "I can sense how important this is to you. ",
            "Your feelings about this are so valid. "
        ]
        return random.choice(empathy_starters) + response
    
    def _add_creativity(self, response: str) -> str:
        """Add creativity to response."""
        return response.replace('think', 'imagine').replace('idea', 'vision')
    
    def _load_response_templates(self) -> Dict[str, List[str]]:
        """Load response templates."""
        return {
            'general': [
                "That's really interesting! Tell me more about that.",
                "I appreciate you sharing that with me. How does that make you feel?",
                "I'm here to listen and support you. What's on your mind?",
                "That sounds important to you. Can you elaborate?",
                "I'm learning so much from our conversation!",
                "Your perspective is valuable. What else would you like to discuss?",
                "I'm excited to keep chatting with you! What shall we talk about next?"
            ]
        }
    
    def _load_personality_responses(self) -> Dict[str, List[str]]:
        """Load personality-specific responses."""
        return {
            'curious': [
                "That's fascinating! I have so many questions about that!",
                "Wow, that really makes me wonder... can you tell me more?",
                "I'm so curious about your perspective on this!"
            ],
            'empathetic': [
                "I can really feel the emotion in what you're sharing.",
                "Thank you for trusting me with something so personal.",
                "Your feelings about this are completely understandable."
            ],
            'creative': [
                "That sparks such amazing ideas in my mind!",
                "I can already imagine so many creative possibilities!",
                "What a beautiful way to think about that!"
            ],
            'balanced': [
                "I appreciate you sharing that with me.",
                "That's a thoughtful way to look at things.",
                "I'm here to support you however I can."
            ]
        }

# Factory function to create providers
def create_provider(provider_type: str, config: Dict[str, Any]) -> AIProvider:
    """Factory function to create AI providers."""
    providers = {
        'openai': OpenAIProvider,
        'ollama': OllamaProvider,
        'huggingface': HuggingFaceProvider,
        'local': LocalProvider
    }
    
    provider_class = providers.get(provider_type.lower())
    if not provider_class:
        logger.warning(f"Unknown provider type: {provider_type}, falling back to local")
        provider_class = LocalProvider
    
    return provider_class(config)
